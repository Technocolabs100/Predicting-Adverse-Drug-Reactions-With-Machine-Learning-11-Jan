{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fx4faMrck2ri",
    "outputId": "71fac306-fd34-4dc7-9109-ca173fc7cce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kora in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (0.9.19)\n",
      "Requirement already satisfied: ipython in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from kora) (7.8.0)\n",
      "Requirement already satisfied: fastcore in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from kora) (1.3.19)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from ipython->kora) (0.7.5)\n",
      "Requirement already satisfied: pygments in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from ipython->kora) (2.4.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from ipython->kora) (4.4.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from ipython->kora) (4.3.3)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from ipython->kora) (0.4.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from ipython->kora) (41.4.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from ipython->kora) (2.0.10)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from ipython->kora) (0.15.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from ipython->kora) (0.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from fastcore->kora) (19.2)\n",
      "Requirement already satisfied: pip in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from fastcore->kora) (19.2.3)\n",
      "Requirement already satisfied: six in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython->kora) (1.12.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython->kora) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->kora) (0.1.7)\n",
      "Requirement already satisfied: parso>=0.5.0 in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython->kora) (0.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\senjuti ghosh\\anaconda3\\lib\\site-packages (from packaging->fastcore->kora) (2.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install kora\n",
    "import kora.install.rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "vIP9SEWDmOWC"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "from rdkit import DataStructs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "8lCKkWBymwYr"
   },
   "outputs": [],
   "source": [
    "# auxillary function to create atom pairs\n",
    "def to_numpyarray_to_list(desc):\n",
    "    arr = np.zeros((1,))\n",
    "    DataStructs.ConvertToNumpyArray(desc, arr)\n",
    "    return arr.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "RLwbustum3zu"
   },
   "outputs": [],
   "source": [
    "# function for creating descriptions\n",
    "#importing libraries\n",
    "from rdkit.Chem import Descriptors, Lipinski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "wV9hjATNnCQk"
   },
   "outputs": [],
   "source": [
    "def calc_descriptors(df_molecules, write=False):\n",
    "    \n",
    "    # Making a copy of the molecule dataframe\n",
    "    df_mols_desc = df_molecules.copy()\n",
    "\n",
    "    # Create the descriptors (9)\n",
    "    df_mols_desc[\"molweight\"] = df_mols_desc[\"mols\"].apply(Descriptors.ExactMolWt)\n",
    "    df_mols_desc[\"hatommolwt\"] = df_mols_desc[\"mols\"].apply(Descriptors.HeavyAtomMolWt)\n",
    "    df_mols_desc[\"maxabspartcharge\"] = df_mols_desc[\"mols\"].apply(Descriptors.MaxAbsPartialCharge)\n",
    "    df_mols_desc[\"maxpartcharge\"] = df_mols_desc[\"mols\"].apply(Descriptors.MaxPartialCharge)\n",
    "    df_mols_desc[\"minabspc\"] = df_mols_desc[\"mols\"].apply(Descriptors.MinAbsPartialCharge)\n",
    "    df_mols_desc[\"minpartcharge\"] = df_mols_desc[\"mols\"].apply(Descriptors.MinPartialCharge)\n",
    "    df_mols_desc[\"molwt\"] = df_mols_desc[\"mols\"].apply(Descriptors.MolWt)\n",
    "    df_mols_desc[\"numrade\"] = df_mols_desc[\"mols\"].apply(Descriptors.NumRadicalElectrons)\n",
    "    df_mols_desc[\"numval\"] = df_mols_desc[\"mols\"].apply(Descriptors.NumValenceElectrons)\n",
    "\n",
    "    #Lipinski (18)\n",
    "    df_mols_desc[\"fracsp33\"] = df_mols_desc[\"mols\"].apply(Lipinski.FractionCSP3)\n",
    "    df_mols_desc[\"heavyatomcount\"] = df_mols_desc[\"mols\"].apply(Lipinski.HeavyAtomCount)\n",
    "    df_mols_desc[\"nhohcount\"] = df_mols_desc[\"mols\"].apply(Lipinski.NHOHCount)\n",
    "    df_mols_desc[\"nocount\"] = df_mols_desc[\"mols\"].apply(Lipinski.NOCount)\n",
    "    df_mols_desc[\"aliphcarbocycles\"] = df_mols_desc[\"mols\"].apply(Lipinski.NumAliphaticCarbocycles)\n",
    "    df_mols_desc[\"aliphhetcycles\"] = df_mols_desc[\"mols\"].apply(Lipinski.NumAliphaticHeterocycles)\n",
    "    df_mols_desc[\"aliphrings\"] = df_mols_desc[\"mols\"].apply(Lipinski.NumAliphaticRings)\n",
    "    df_mols_desc[\"arocarbocycles\"] = df_mols_desc[\"mols\"].apply(Lipinski.NumAromaticCarbocycles)\n",
    "    df_mols_desc[\"arohetcycles\"] = df_mols_desc[\"mols\"].apply(Lipinski.NumAromaticHeterocycles)\n",
    "    df_mols_desc[\"arorings\"] = df_mols_desc[\"mols\"].apply(Lipinski.NumAromaticRings)\n",
    "    df_mols_desc[\"numhacceptors\"] = df_mols_desc[\"mols\"].apply(Lipinski.NumHAcceptors)\n",
    "    df_mols_desc[\"numhdonors\"] = df_mols_desc[\"mols\"].apply(Lipinski.NumHDonors)\n",
    "    df_mols_desc[\"numhatoms\"] = df_mols_desc[\"mols\"].apply(Lipinski.NumHeteroatoms)\n",
    "    df_mols_desc[\"numrotbonds\"] = df_mols_desc[\"mols\"].apply(Lipinski.NumRotatableBonds)\n",
    "    df_mols_desc[\"numsatcarbcycles\"] = df_mols_desc[\"mols\"].apply(Lipinski.NumSaturatedCarbocycles)\n",
    "    df_mols_desc[\"numsathetcycles\"] = df_mols_desc[\"mols\"].apply(Lipinski.NumSaturatedHeterocycles)\n",
    "    df_mols_desc[\"numsatrings\"] = df_mols_desc[\"mols\"].apply(Lipinski.NumSaturatedRings)\n",
    "    df_mols_desc[\"ringcount\"] = df_mols_desc[\"mols\"].apply(Lipinski.RingCount)\n",
    "\n",
    "    #Drop SMILES and MOLS\n",
    "    df_mols_desc.drop(\"mols\", inplace=True, axis=1)\n",
    "\n",
    "\n",
    "    #Fill NaN with 0\n",
    "    df_mols_desc = df_mols_desc.fillna(0)\n",
    "    df_mols_desc.to_csv(\"df_mols_desc.csv\")\n",
    "\n",
    "    return df_mols_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "9Uwc9WI2nQdR"
   },
   "outputs": [],
   "source": [
    "# creating fingerprints\n",
    "# importing libraries\n",
    "from rdkit.Chem import rdMolDescriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "_QvLaBB-neWf"
   },
   "outputs": [],
   "source": [
    "def get_morgan(molecule, length=512):\n",
    "    try:\n",
    "        # radius=2 = ECFP4, radius=3 = ECFP6, etc.\n",
    "        desc = rdMolDescriptors.GetMorganFingerprintAsBitVect(molecule, 2, nBits=length)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('error ' + str(molecule))\n",
    "        desc = np.nan\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "XKqv6YAinlKl"
   },
   "outputs": [],
   "source": [
    "def get_maccs(molecule):\n",
    "    try:\n",
    "        maccs = rdMolDescriptors.GetMACCSKeysFingerprint(molecule)\n",
    "        # Does not have length\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"error\" + str(molecule))\n",
    "        maccs = np.nan\n",
    "    return maccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "G4obQzR_nn17"
   },
   "outputs": [],
   "source": [
    "def get_atompairs(molecule, length=512):\n",
    "    try:\n",
    "        atompairs = rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(molecule, nBits=length)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"error\" + str(molecule))\n",
    "        atompairs = np.nan\n",
    "    return atompairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "XAksRtionrJ7"
   },
   "outputs": [],
   "source": [
    "def get_topological_torsion(molecule, length=512):\n",
    "    try:\n",
    "        tt = rdMolDescriptors.GetHashedTopologicalTorsionFingerprintAsBitVect(molecule, nBits=length)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"error\" + str(molecule))\n",
    "        tt = np.nan\n",
    "    return tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "8EDTO6Cuntqt"
   },
   "outputs": [],
   "source": [
    "def create_ecfp4_fingerprint(df_molecules, length=512, write=False):\n",
    "    # Morgan Fingerprint (ECFP4)\n",
    "    df_w = df_molecules.copy()\n",
    "    df_w[\"ECFP4\"] = df_w[\"mols\"].apply(lambda x: get_morgan(x, length)).apply(to_numpyarray_to_list)\n",
    "\n",
    "    # New DF with one column for each ECFP bit\n",
    "    ecfp_df = df_w['ECFP4'].apply(pd.Series)\n",
    "    ecfp_df = ecfp_df.rename(columns=lambda x: 'ECFP4_' + str(x + 1))\n",
    "\n",
    "    # Write to csv\n",
    "    ecfp_df.to_csv(\"ecfp4.csv\")\n",
    "\n",
    "    return ecfp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "eBtVpIgzn0XG"
   },
   "outputs": [],
   "source": [
    "def create_maccs_fingerprint(df_molecules, write=False):\n",
    "    # MACCS keys\n",
    "    df_w = df_molecules.copy()\n",
    "    df_w[\"MACCS\"] = df_w[\"mols\"].apply(get_maccs).apply(to_numpyarray_to_list)\n",
    "\n",
    "    # New DF with one column for each MACCS key\n",
    "    maccs_df = df_w['MACCS'].apply(pd.Series)\n",
    "    maccs_df = maccs_df.rename(columns=lambda x: 'MACCS_' + str(x + 1))\n",
    "\n",
    "    # Write to csv\n",
    "    maccs_df.to_csv(\"maccs.csv\")\n",
    "\n",
    "    return maccs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "UUS0iKoFn9G5"
   },
   "outputs": [],
   "source": [
    "def create_atompairs_fingerprint(df_molecules, length=512, write=False):\n",
    "    # ATOM PAIRS\n",
    "    df_w = df_molecules.copy()\n",
    "    df_w[\"ATOMPAIRS\"] = df_w[\"mols\"].apply(lambda x: get_atompairs(x, length)).apply(\n",
    "        to_numpyarray_to_list)\n",
    "\n",
    "    # New DF with one column for each ATOM PAIRS key\n",
    "    atom_pairs_df = df_w['ATOMPAIRS'].apply(pd.Series)\n",
    "    atom_pairs_df = atom_pairs_df.rename(columns=lambda x: 'ATOMPAIR_' + str(x + 1))\n",
    "\n",
    "    # Write to csv\n",
    "    atom_pairs_df.to_csv(\"atom_pairs.csv\")\n",
    "\n",
    "    return atom_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "4HC5tBYpoELE"
   },
   "outputs": [],
   "source": [
    "def create_topological_torsion_fingerprint(df_molecules, length=512, write=False):\n",
    "    \n",
    "    # Topological Torsion\n",
    "    df_w = df_molecules.copy()\n",
    "    df_w[\"TT\"] = df_w[\"mols\"].apply(lambda x: get_topological_torsion(x, length)).apply(to_numpyarray_to_list)\n",
    "\n",
    "    # New DF with one column for each Topological torsion key\n",
    "    tt_df = df_w['TT'].apply(pd.Series)\n",
    "    tt_df = tt_df.rename(columns=lambda x: 'TT' + str(x + 1))\n",
    "\n",
    "    # Write to csv\n",
    "    tt_df.to_csv(\"topological_torsion.csv\")\n",
    "\n",
    "    return tt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MoKRm9goL-L"
   },
   "outputs": [],
   "source": [
    "# pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "cG6RF7_ZoWLT"
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eijx81pjoYyz",
    "outputId": "0cf9179e-edd7-4048-b0d8-8e12cde749d0"
   },
   "outputs": [],
   "source": [
    "# Misc\n",
    "from rdkit import Chem\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, \\\n",
    "    roc_auc_score, precision_recall_curve, average_precision_score\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from collections import Counter\n",
    "import re, requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "GFwe4TEXofnM"
   },
   "outputs": [],
   "source": [
    "def create_original_df(usedf=False, file=None, write_s=False, write_off=False):\n",
    "    \n",
    "    # Create dataframe from csv\n",
    "    if not usedf:\n",
    "        df = pd.read_csv(\"C:/Users/SENJUTI GHOSH/Technocolabs_PredictingAdverseReactions/datasets/sider.csv\", skipinitialspace=True)\n",
    "    else:\n",
    "        df = file.copy()\n",
    "\n",
    "    # Extract SMILES column\n",
    "    df_molecules = pd.DataFrame(df[\"smiles\"])\n",
    "\n",
    "    # Converting to molecules\n",
    "    df_molecules[\"mols\"] = df_molecules[\"smiles\"].apply(Chem.MolFromSmiles)\n",
    "\n",
    "    # Droping mols and smiles\n",
    "    df_y = df.drop(\"smiles\", axis=1)\n",
    "\n",
    "    # Write to csv\n",
    "    df_molecules.to_csv(\"df_molecules.csv\")\n",
    "    df_y.to_csv(\"df_y.csv\")\n",
    "\n",
    "    df_molecules.to_csv(\"df_off_mols.csv\")\n",
    "    df_y.to_csv(\"df_off_y.csv\")\n",
    "\n",
    "    return df_y, df_molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "EBBR2ljgoz3Z"
   },
   "outputs": [],
   "source": [
    "def createfingerprints(df_mols, length): # using fingerprints functions\n",
    "    \n",
    "    # Morgan Fingerprint (ECFP4)\n",
    "    ecfp_df = create_ecfp4_fingerprint(df_mols, length, False)\n",
    "\n",
    "    # MACCS keys (always 167)\n",
    "    maccs_df = create_maccs_fingerprint(df_mols, False)\n",
    "\n",
    "    # ATOM PAIRS\n",
    "    atom_pairs_df = create_atompairs_fingerprint(df_mols, length, False)\n",
    "\n",
    "    # Topological torsion\n",
    "    tt_df = create_topological_torsion_fingerprint(df_mols, length, False)\n",
    "\n",
    "    return ecfp_df, maccs_df, atom_pairs_df, tt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "0egTUHgJpB1U"
   },
   "outputs": [],
   "source": [
    "def createdescriptors(df_molecules): # using descriptions function\n",
    "    \n",
    "    # Descriptors\n",
    "    df_mols_desc = calc_descriptors(df_molecules, False)\n",
    "\n",
    "    return df_mols_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "vexZNsX9pJkq"
   },
   "outputs": [],
   "source": [
    "def test_fingerprint_size(df_mols, df_y, model, colname=\"Hepatobiliary disorders\", num_sizes_to_test=20, min_size=100,\n",
    "                          max_size=2048, cv=10, makeplots=False, write=False):\n",
    "    \n",
    "    # Fingerprint length type and selection\n",
    "    # Scoring metrics to use\n",
    "    scoring_metrics = (\"f1_micro\", \"f1_macro\", \"f1\", \"roc_auc\", \"recall\", \"precision\", \"average_precision\")\n",
    "    sizes = np.linspace(min_size, max_size, num_sizes_to_test, dtype=int)\n",
    "\n",
    "    # Create results dataframes for each metric\n",
    "    results_f1 = np.zeros([4, len(sizes)])\n",
    "    results_rocauc = np.zeros([4, len(sizes)])\n",
    "    results_precision = np.zeros([4, len(sizes)])\n",
    "    results_recall = np.zeros([4, len(sizes)])\n",
    "    results_average_precision = np.zeros([4, len(sizes)])\n",
    "    results_f1_micro = np.zeros([4, len(sizes)])\n",
    "    results_f1_macro = np.zeros([4, len(sizes)])\n",
    "\n",
    "    # Get test sizes\n",
    "    c = 0\n",
    "    \n",
    "    # Size testing using SVC with scale gamma (1 / (n_features * X.var()))\n",
    "    for s in tqdm(sizes):\n",
    "        \n",
    "        # Create fingerprint with size S\n",
    "        fingerprints = createfingerprints(df_mols, int(s))\n",
    "        r = 0\n",
    "        for fp in fingerprints:\n",
    "            X = fp.copy()\n",
    "            \n",
    "            # Using \"Hepatobiliary disorders\" as an results example since its balanced\n",
    "            y = df_y[colname].copy()\n",
    "            \n",
    "            # 10-fold cross validation\n",
    "            cv_scores = cross_validate(model, X, y, cv=cv, scoring=scoring_metrics, return_train_score=False, n_jobs=-1)\n",
    "\n",
    "            for k, v in cv_scores.items():\n",
    "                if k == \"test_roc_auc\":\n",
    "                    results_rocauc[r, c] = v.mean()\n",
    "                if k == \"test_precision\":\n",
    "                    results_precision[r, c] = v.mean()\n",
    "                if k == \"test_recall\":\n",
    "                    results_recall[r, c] = v.mean()\n",
    "                if k == \"test_average_precision\":\n",
    "                    results_average_precision[r, c] = v.mean()\n",
    "                if k == \"test_f1\":\n",
    "                    results_f1[r, c] = v.mean()\n",
    "                if k == \"test_f1_micro\":\n",
    "                    results_f1_micro[r, c] = v.mean()\n",
    "                if k == \"test_f1_macro\":\n",
    "                    results_f1_macro[r, c] = v.mean()\n",
    "            r += 1\n",
    "        c += 1\n",
    "\n",
    "    all_results = (results_rocauc, results_precision, results_recall, results_average_precision, results_f1,\n",
    "                   results_f1_micro, results_f1_macro)\n",
    "\n",
    "    # Create dataframe for results\n",
    "    df_results_rocauc_size_SVC = pd.DataFrame(results_rocauc, columns=sizes)\n",
    "    df_results_precision_size_SVC = pd.DataFrame(results_precision, columns=sizes)\n",
    "    df_results_recall_size_SVC = pd.DataFrame(results_recall, columns=sizes)\n",
    "    df_results_av_prec_size_SVC = pd.DataFrame(results_average_precision, columns=sizes)\n",
    "    df_results_f1_size_SVC = pd.DataFrame(results_f1, columns=sizes)\n",
    "    df_results_f1_micro_size_SVC = pd.DataFrame(results_f1_micro, columns=sizes)\n",
    "    df_results_f1_macro_size_SVC = pd.DataFrame(results_f1_macro, columns=sizes)\n",
    "\n",
    "    all_df_results = (\n",
    "        df_results_rocauc_size_SVC, df_results_precision_size_SVC, df_results_recall_size_SVC,\n",
    "        df_results_av_prec_size_SVC, df_results_f1_size_SVC, df_results_f1_micro_size_SVC, df_results_f1_macro_size_SVC)\n",
    "\n",
    "    # Save to file\n",
    "    df_results_rocauc_size_SVC.to_csv(\"df_results_rocauc_size_SVC.csv\")\n",
    "    df_results_precision_size_SVC.to_csv(\"df_results_precision_size_SVC.csv\")\n",
    "    df_results_recall_size_SVC.to_csv(\"df_results_recall_size_SVC.csv\")\n",
    "    df_results_av_prec_size_SVC.to_csv(\"df_results_av_prec_size_SVC.csv\")\n",
    "    df_results_f1_size_SVC.to_csv(\"df_results_f1_size_SVC.csv\")\n",
    "    df_results_f1_micro_size_SVC.to_csv(\"df_results_f1_micro_size_SVC.csv\")\n",
    "    df_results_f1_macro_size_SVC.to_csv(\"df_results_f1_macro_size_SVC.csv\")\n",
    "\n",
    "    if makeplots:\n",
    "        fp_names = [\"ECFP-4\", \"MACCS\", \"Atom Pairs\", \"Topological Torsion\"]\n",
    "        m = 0\n",
    "        for d in all_results:\n",
    "            fig = plt.figure(figsize=(10, 10))\n",
    "            for i in range(len(fingerprints)):\n",
    "                plt.plot(sizes, d[i, :], \"-\")\n",
    "            plt.title(f\"SVC, {scoring_metrics[m]} vs fingerprint length\", fontsize=25)\n",
    "            plt.ylabel(f\"{scoring_metrics[m]}\", fontsize=20)\n",
    "            plt.xlabel(\"Fingerprint Length\", fontsize=20)\n",
    "            plt.legend(fp_names, fontsize=15)\n",
    "            plt.ylim([0, 1])\n",
    "            plt.show()\n",
    "            m += 1\n",
    "\n",
    "    return all_df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "48vj-0PGpq88"
   },
   "outputs": [],
   "source": [
    "def select_best_descriptors_multi(df_desc, y_all, out_names=[], score_func=f_classif, k=1):\n",
    "    \n",
    "    # Select k highest scoring feature from X to every y and return new df with only the selected ones\n",
    "    if not out_names:\n",
    "        print(\"Column names necessary\")\n",
    "        return None\n",
    "    selected = []\n",
    "    for n in tqdm(out_names):\n",
    "        skb = SelectKBest(score_func=score_func, k=k).fit(df_desc, y_all[n])\n",
    "        n_sel_bol = skb.get_support()\n",
    "        sel = df_desc.loc[:, n_sel_bol].columns.to_list()\n",
    "        for s in sel:\n",
    "            if s not in selected:\n",
    "                selected.append(s)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "le7CyKJbpvMI"
   },
   "outputs": [],
   "source": [
    "def select_best_descriptors(X, y, score_func=f_classif, k=2):\n",
    "    \n",
    "    # Select k highest scoring feature from X to y with a score function, f_classif by default\n",
    "    skb = SelectKBest(score_func=score_func, k=k).fit(X, y)\n",
    "    n_sel_bol = skb.get_support()\n",
    "    sel = X.loc[:, n_sel_bol].columns.to_list()\n",
    "    assert sel\n",
    "    return sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "y0RYrMvupyeF"
   },
   "outputs": [],
   "source": [
    "def create_dataframes_dic(df_desc_base_train, df_desc_base_test, X_train_fp, X_test_fp, y_train, out_names,\n",
    "                          score_func=f_classif, k=3):\n",
    "    # Create 3 dictionaries, one with the train dataframes, one with the test dataframes and one with the selected\n",
    "    # features for each label\n",
    "\n",
    "    # Initialize dictonaries\n",
    "    train_series_dic = {name: None for name in out_names}\n",
    "    test_series_dic = {name: None for name in out_names}\n",
    "    selected_name = {name: None for name in out_names}\n",
    "\n",
    "    # For each of the tasks build the train and test dataframe with the selected descriptors\n",
    "    for name in tqdm(out_names):\n",
    "        \n",
    "        # Select best descriptors for the task\n",
    "        sel_col = select_best_descriptors(df_desc_base_train, y_train[name], score_func=score_func, k=k)\n",
    "        selected_name[name] = sel_col  # Keep track of selected columns\n",
    "        df_desc_train = df_desc_base_train.loc[:, sel_col].copy()  # Get train dataframe with only selected columns\n",
    "        df_desc_test = df_desc_base_test.loc[:, sel_col].copy()  # Get test dataframe with only selected columns\n",
    "        X_train = pd.concat([X_train_fp, df_desc_train], axis=1)\n",
    "        X_test = pd.concat([X_test_fp, df_desc_test], axis=1)\n",
    "        \n",
    "        # Add to the dictionary\n",
    "        train_series_dic[name] = X_train\n",
    "        test_series_dic[name] = X_test\n",
    "\n",
    "    # Return the dictionaries\n",
    "    return train_series_dic, test_series_dic, selected_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "WEsue618p65Q"
   },
   "outputs": [],
   "source": [
    "def balance_dataset(X_train_dic, y_train_dic, out_names, random_state=0, n_jobs=-1, verbose=False):\n",
    "    \n",
    "    # Initialize the dictionaries and boolean array for categorical features\n",
    "    train_series_dic_bal = {name: None for name in out_names}\n",
    "    y_dic_bal = {name: None for name in out_names}\n",
    "    cat_shape = np.full((1128,), True, dtype=bool)\n",
    "    cat_shape[-3:] = False\n",
    "\n",
    "    # For each classficiation label\n",
    "    for label in tqdm(out_names):\n",
    "        X_imb = X_train_dic[label]\n",
    "        y_imb = y_train_dic[label]\n",
    "        X_bal, y_bal = SMOTENC(categorical_features=cat_shape, random_state=random_state, n_jobs=n_jobs).fit_resample(\n",
    "            X_imb, y_imb)\n",
    "        train_series_dic_bal[label] = X_bal\n",
    "        y_dic_bal[label] = y_bal\n",
    "\n",
    "    # Print new counts\n",
    "    if verbose:\n",
    "        for label in out_names:\n",
    "            print(f\"For {label}\")\n",
    "            print(sorted(Counter(y_train_dic[label]).items()))\n",
    "            print(sorted(Counter(y_dic_bal[label]).items()))\n",
    "\n",
    "    # Return the new dictionaries\n",
    "    return train_series_dic_bal, y_dic_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "8x63aoRip_6C"
   },
   "outputs": [],
   "source": [
    "def grid_search(X_train, y_train, model, params_to_test, X_test=None, y_test=None, balancing=False, n_splits=5,\n",
    "                scoring=\"f1\", n_jobs=-1, verbose=False, random_state=None):\n",
    "    \n",
    "    # Define grid search\n",
    "    if balancing:\n",
    "        \n",
    "        # Save index of categorical features\n",
    "        cat_shape = np.full((1128,), True, dtype=bool)\n",
    "        cat_shape[-3:] = False\n",
    "        \n",
    "        # Prepatre SMOTENC\n",
    "        smotenc = SMOTENC(categorical_features=cat_shape, random_state=random_state, n_jobs=n_jobs)\n",
    "        \n",
    "        # Make a pipeline with the balancing and the estimator, balacing is only called when fitting\n",
    "        pipeline = make_pipeline(smotenc, model)\n",
    "        \n",
    "        # Determine stratified k folds\n",
    "        kf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "        \n",
    "        # Call cross validate\n",
    "        grid_search = GridSearchCV(pipeline, params_to_test, cv=kf, n_jobs=n_jobs, verbose=verbose, scoring=scoring)\n",
    "\n",
    "    else:\n",
    "        kf = StratifiedKFold(n_splits=n_splits, random_state=random_state)\n",
    "        grid_search = GridSearchCV(model, params_to_test, cv=kf, n_jobs=n_jobs, verbose=verbose, scoring=scoring)\n",
    "\n",
    "    # Fit X and y to test parameters\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    means = grid_search.cv_results_[\"mean_test_score\"]\n",
    "    stds = grid_search.cv_results_[\"std_test_score\"]\n",
    "\n",
    "    if verbose:\n",
    "        \n",
    "        # Print scores\n",
    "        print()\n",
    "        print(\"Score for development set:\")\n",
    "        for mean, std, params in zip(means, stds, grid_search.cv_results_[\"params\"]):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 1.96, params))\n",
    "        print()\n",
    "\n",
    "        # Print best parameters\n",
    "        print()\n",
    "        print(\"Best parameters set found:\")\n",
    "        print(grid_search.best_params_)\n",
    "        print()\n",
    "        if X_test and y_test:\n",
    "            \n",
    "            # Detailed Classification report\n",
    "            print()\n",
    "            print(\"Detailed classification report:\")\n",
    "            print(\"The model is trained on the full development set.\")\n",
    "            print(\"The scores are computed on the full evaluation set.\")\n",
    "            print()\n",
    "            y_true, y_pred = y_test, grid_search.predict(X_test)\n",
    "            print(classification_report(y_true, y_pred))\n",
    "            print()\n",
    "            print(\"Confusion Matrix as\")\n",
    "            print(\"\"\"\n",
    "            TN FP\n",
    "            FN TP\n",
    "            \"\"\")\n",
    "            print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    # Save best estimator\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    # And return it\n",
    "    return best_params, best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "sJuCjUo4qMuF"
   },
   "outputs": [],
   "source": [
    "def multi_label_grid_search(X_train_dic, y_train, out_names, model, params_to_test, balancing=False, X_test=None,\n",
    "                            y_test=None, n_splits=5, scoring=\"f1\", n_jobs=-1, verbose=False, random_state=None):\n",
    "    \n",
    "    # Creates a dictionary with the best params in regards to chosen metric for each label\n",
    "\n",
    "    # Creates the dictionary\n",
    "    best_params_by_label = {label: None for label in out_names}\n",
    "\n",
    "    # If X_test and y_test is given so that generalization evalutation can happen\n",
    "    if X_test and y_test:\n",
    "        for label in tqdm(out_names):\n",
    "            print()\n",
    "            print(f\"Scores for {label}\")\n",
    "            best_params, _ = grid_search(X_train_dic[label], y_train[label], model, params_to_test[label],\n",
    "                                         X_test[label], y_test[label], n_splits=n_splits, scoring=scoring,\n",
    "                                         verbose=verbose, n_jobs=n_jobs, balancing=balancing, random_state=random_state)\n",
    "            best_params_by_label[label] = best_params\n",
    "    else:\n",
    "        for label in tqdm(out_names):\n",
    "            print()\n",
    "            print(f\"Scores for {label}\")\n",
    "            best_params, _ = grid_search(X_train_dic[label], y_train[label], model, params_to_test[label],\n",
    "                                         n_splits=n_splits, scoring=scoring, verbose=verbose, n_jobs=n_jobs,\n",
    "                                         balancing=balancing, random_state=random_state)\n",
    "            best_params_by_label[label] = best_params\n",
    "\n",
    "    return best_params_by_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "mFn8LwCZqRux"
   },
   "outputs": [],
   "source": [
    "def random_search(X_train, y_train, model, params_to_test, X_test=None, y_test=None, balancing=False,\n",
    "                  n_iter=100, n_splits=5, scoring=\"f1\", n_jobs=-1, verbose=False, random_state=None):\n",
    "    \n",
    "    # Define random search\n",
    "    if balancing:\n",
    "        \n",
    "        # Save index of categorical features\n",
    "        cat_shape = np.full((1128,), True, dtype=bool)\n",
    "        cat_shape[-3:] = False\n",
    "        \n",
    "        # Prepatre SMOTENC\n",
    "        smotenc = SMOTENC(categorical_features=cat_shape, random_state=random_state, n_jobs=n_jobs)\n",
    "        \n",
    "        # Make a pipeline with the balancing and the estimator, balacing is only called when fitting\n",
    "        pipeline = make_pipeline(smotenc, model)\n",
    "        \n",
    "        # Determine stratified k folds\n",
    "        kf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "        \n",
    "        # Call cross validate\n",
    "        rs = RandomizedSearchCV(pipeline, params_to_test, n_iter=n_iter, cv=kf, n_jobs=n_jobs, verbose=verbose,\n",
    "                                scoring=scoring)\n",
    "\n",
    "    else:\n",
    "        kf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "        rs = RandomizedSearchCV(model, params_to_test, n_iter=n_iter, cv=kf, n_jobs=n_jobs, verbose=verbose,\n",
    "                                scoring=scoring)\n",
    "\n",
    "    # Fit parameters\n",
    "    rs.fit(np.asarray(X_train), np.asarray(y_train))\n",
    "    means = rs.cv_results_[\"mean_test_score\"]\n",
    "    stds = rs.cv_results_[\"std_test_score\"]\n",
    "\n",
    "    # Print scores\n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"Score for development set:\")\n",
    "\n",
    "        for mean, std, params in zip(means, stds, rs.cv_results_[\"params\"]):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 1.96, params))\n",
    "        print()\n",
    "\n",
    "        # Print best parameters\n",
    "        print()\n",
    "        print(\"Best parameters set found:\")\n",
    "        print(rs.best_params_)\n",
    "        print()\n",
    "        if X_test and y_test:\n",
    "            \n",
    "            # Detailed Classification report\n",
    "\n",
    "            print()\n",
    "            print(\"Detailed classification report:\")\n",
    "            print(\"The model is trained on the full development set.\")\n",
    "            print(\"The scores are computed on the full evaluation set.\")\n",
    "            print()\n",
    "            y_true, y_pred = y_test, rs.predict(X_test)\n",
    "            print(classification_report(y_true, y_pred))\n",
    "            print()\n",
    "            \"\"\"\n",
    "            print(\"Confusion matrix as:\")\n",
    "            print(\n",
    "                   TN FP\n",
    "                   FN TP\n",
    "                   )\n",
    "            print(confusion_matrix(y_true, y_pred))\n",
    "            print()\n",
    "            \"\"\"\n",
    "    # Save best estimator\n",
    "    best_estimator = rs.best_estimator_\n",
    "    best_params = rs.best_params_\n",
    "    \n",
    "    # And return it\n",
    "    return best_params, best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "HPAEHy5XqcvH"
   },
   "outputs": [],
   "source": [
    "def multi_label_random_search(X_train_dic, y_train, out_names, model, params_to_test, balancing=False, X_test=None,\n",
    "                              y_test=None, n_iter=100, n_splits=5, scoring=\"f1\", n_jobs=-1, verbose=False,\n",
    "                              random_state=None):\n",
    "    # Creates a dictionary with the best params in regards to chosen metric for each label\n",
    "\n",
    "    # Creates the dictionary\n",
    "    best_params_by_label = {label: None for label in out_names}\n",
    "\n",
    "    # If X_test and y_test is given so that generalization evalutation can happen\n",
    "    if X_test and y_test:\n",
    "        for label in tqdm(out_names):\n",
    "            print()\n",
    "            print(f\"Scores for {label}\")\n",
    "            best_params, _ = random_search(X_train_dic[label], y_train[label], model, params_to_test[label],\n",
    "                                           X_test[label], y_test[label], n_iter=n_iter, n_splits=n_splits,\n",
    "                                           scoring=scoring, verbose=verbose, n_jobs=n_jobs, random_state=random_state,\n",
    "                                           balancing=balancing)\n",
    "            best_params_by_label[label] = best_params\n",
    "    else:\n",
    "        for label in tqdm(out_names):\n",
    "            print()\n",
    "            print(f\"Scores for {label}\")\n",
    "            best_params, _ = random_search(X_train_dic[label], y_train[label], model, params_to_test[label],\n",
    "                                           n_iter=n_iter, n_splits=n_splits, scoring=scoring, verbose=verbose,\n",
    "                                           n_jobs=n_jobs, random_state=random_state, balancing=balancing)\n",
    "            best_params_by_label[label] = best_params\n",
    "\n",
    "    return best_params_by_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "-pn_8ENvqgrC"
   },
   "outputs": [],
   "source": [
    "def score_report(estimator, X_test, y_test, verbose=False, plot=False, name=None):\n",
    "    \n",
    "    # Predicting value\n",
    "    y_true, y_pred = y_test, estimator.predict(X_test)\n",
    "    y_score = estimator.predict_proba(X_test)\n",
    "    y_score = y_score[:, 1]\n",
    "\n",
    "    # Individual metrics\n",
    "    f1_micr_score = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    f1_macro_score = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    f1_s_score = f1_score(y_true, y_pred, average=\"binary\")\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred, average=\"binary\")\n",
    "    prec = precision_score(y_true, y_pred, average=\"binary\")\n",
    "    average_precision = average_precision_score(y_true, y_score)\n",
    "\n",
    "    # Detailed Classification report\n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"The scores are computed on the full evaluation set\")\n",
    "        print(\"These are not used to train or optimize the model\")\n",
    "        print()\n",
    "        print(\"Detailed classification report:\")\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        print()\n",
    "\n",
    "        print(\"Confusion matrix as:\")\n",
    "        print(\"\"\"\n",
    "               TN FP\n",
    "               FN TP\n",
    "               \"\"\")\n",
    "        print(confusion_matrix(y_true, y_pred))\n",
    "        print()\n",
    "\n",
    "        print(\"Individual metrics:\")\n",
    "        print(f\"F1 Micro score: {f1_micr_score:.3f}\")\n",
    "        print(f\"F1 Macro score: {f1_macro_score:.3f}\")\n",
    "        print(f\"F1 Binary score: {f1_s_score:.3f}\")\n",
    "        print(f\"AUROC score: {auc:.3f}\")\n",
    "        print(f\"Recall score: {rec:.3f}\")\n",
    "        print(f\"Precision score: {prec:.3f}\")\n",
    "        print(f\"Average precision-recall score: {average_precision:.3f}\")\n",
    "        print()\n",
    "\n",
    "    if plot:\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "\n",
    "        # step_kwargs = ({'step': 'post'}\n",
    "        #                if 'step' in signature(plt.fill_between).parameters\n",
    "        #                else {})\n",
    "\n",
    "        plt.step(recall, precision, color=\"r\", alpha=0.2, where=\"post\")\n",
    "        plt.fill_between(recall, precision, step=\"post\", alpha=0.2, color=\"#F59B00\")\n",
    "\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.title(f'{name} \\n Precision-Recall curve: AP={average_precision:0.2f}')\n",
    "\n",
    "        plt.savefig(f\"Precision-Recall curve.png\")\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "    return {\"f1_micr_score\": f1_micr_score, \"auc_score\": auc, \"rec_score\": rec, \"prec_score\": prec,\n",
    "            \"f1_macro_score\": f1_macro_score, \"f1_s_score\": f1_s_score, \"prec_rec_score\": average_precision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "rjk_-s_Yqng7"
   },
   "outputs": [],
   "source": [
    "def cv_report(estimator, X_train, y_train, balancing=False, n_splits=5,\n",
    "              scoring_metrics=(\"f1_micro\", \"f1_macro\", \"f1\", \"roc_auc\", \"recall\", \"precision\", \"average_precision\"),\n",
    "              random_state=None, n_jobs=-1, verbose=False):\n",
    "    if balancing:\n",
    "        \n",
    "        # Save index of categorical features\n",
    "        cat_shape = np.full((1128,), True, dtype=bool)\n",
    "        cat_shape[-3:] = False\n",
    "        \n",
    "        # Prepare SMOTENC\n",
    "        smotenc = SMOTENC(categorical_features=cat_shape, random_state=random_state, n_jobs=n_jobs)\n",
    "        \n",
    "        # Make a pipeline with the balancing and the estimator, balacing is only called when fitting\n",
    "        pipeline = make_pipeline(smotenc, estimator)\n",
    "        \n",
    "        # Determine stratified k folds\n",
    "        kf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "        \n",
    "        # Call cross validate\n",
    "        scores = cross_validate(pipeline, np.asarray(X_train), np.asarray(y_train), scoring=scoring_metrics, cv=kf,\n",
    "                                n_jobs=n_jobs, verbose=verbose, return_train_score=False)\n",
    "\n",
    "    else:\n",
    "        # Normal cross validation\n",
    "        kf = StratifiedKFold(n_splits=n_splits, random_state=random_state,shuffle=True)\n",
    "        scores = cross_validate(estimator, np.asarray(X_train), np.asarray(y_train), scoring=scoring_metrics, cv=kf,\n",
    "                                n_jobs=n_jobs, verbose=verbose, return_train_score=False)\n",
    "\n",
    "    # Means\n",
    "    f1_s = np.mean(scores[\"test_f1_micro\"])\n",
    "    f1_ms = np.mean(scores[\"test_f1_macro\"])\n",
    "    f1_bs = np.mean(scores[\"test_f1\"])\n",
    "    auc_s = np.mean(scores[\"test_roc_auc\"])\n",
    "    rec_s = np.mean(scores[\"test_recall\"])\n",
    "    prec_s = np.mean(scores[\"test_precision\"])\n",
    "    avp_s = np.mean(scores[\"test_average_precision\"])\n",
    "\n",
    "    # STD\n",
    "    f1_std = np.std(scores[\"test_f1_micro\"])\n",
    "    f1_mstd = np.std(scores[\"test_f1_macro\"])\n",
    "    f1_bstd = np.std(scores[\"test_f1\"])\n",
    "    auc_std = np.std(scores[\"test_roc_auc\"])\n",
    "    rec_std = np.std(scores[\"test_recall\"])\n",
    "    prec_std = np.std(scores[\"test_precision\"])\n",
    "    avp_std = np.std(scores[\"test_average_precision\"])\n",
    "\n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"Individual metrics\")\n",
    "        print(f\"F1 Micro Score: Mean: {f1_s:.3f} (Std: {f1_std:.3f})\")\n",
    "        print(f\"F1 Macro Score: Mean: {f1_ms:.3f} (Std: {f1_mstd:.3f})\")\n",
    "        print(f\"F1 Binary Score: Mean: {f1_bs:.3f} (Std: {f1_bstd:.3f})\")\n",
    "        print(f\"AUROC score: Mean: {auc_s:.3f} (Std: {auc_std:.3f})\")\n",
    "        print(f\"Recall score: Mean: {rec_s:.3f} (Std: {rec_std:.3f})\")\n",
    "        print(f\"Precision score: Mean: {prec_s:.3f} (Std: {prec_std:.3f})\")\n",
    "        print(f\"Average Precision score: Mean: {avp_s:.3f} (Std: {avp_std:.3f})\")\n",
    "        print()\n",
    "\n",
    "    return {\"f1_micr_score\": f1_s, \"f1_micr_std\": f1_std, \"auc_score\": auc_s, \"auc_std\": auc_std, \"rec_score\": rec_s,\n",
    "            \"rec_std\": rec_std, \"prec_score\": prec_s, \"prec_std\": prec_std, \"f1_macro_score\": f1_ms,\n",
    "            \"f1_macro_std\": f1_mstd, \"f1_score\": f1_bs, \"f1_std\": f1_bstd, \"avp_score\": avp_s, \"avp_std\": avp_std}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "G2f47LLUqxOb"
   },
   "outputs": [],
   "source": [
    "def cv_multi_report(X_train_dic, y_train, out_names, model=None, balancing=False, modelname=None, spec_params=None,\n",
    "                    random_state=None, n_splits=5, n_jobs=-1, verbose=False):\n",
    "    \n",
    "    # Creates a scores report dataframe for each classification label with cv\n",
    "    # Initizalize the dataframe\n",
    "    report = pd.DataFrame(\n",
    "        columns=[\"F1 Binary\", \"F1 Micro\", \"F1 Macro\", \"ROC_AUC\", \"Recall\", \"Precision\", \"Average Precision\"],\n",
    "        index=out_names)\n",
    "    scoring_metrics = (\"f1_micro\", \"f1_macro\", \"f1\", \"roc_auc\", \"recall\", \"precision\", \"average_precision\")\n",
    "\n",
    "    # For each label\n",
    "    for name in tqdm(out_names):\n",
    "        if verbose:\n",
    "            print()\n",
    "            print(f\"Scores for {name}\")\n",
    "        \n",
    "        # Calculate the score for the current label using the respective dataframe\n",
    "        if spec_params:\n",
    "            \n",
    "            # Define the specific parameters for each model for each label\n",
    "            if modelname[name] == \"SVC\":\n",
    "                model_temp = SVC(random_state=random_state, probability=True)\n",
    "                model_temp.set_params(C=spec_params[name][\"svc__C\"],\n",
    "                                      gamma=spec_params[name][\"svc__gamma\"],\n",
    "                                      kernel=spec_params[name][\"svc__kernel\"])\n",
    "            elif modelname[name] == \"RF\":\n",
    "                model_temp = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "                model_temp.set_params(bootstrap=spec_params[name][\"randomforestclassifier__bootstrap\"],\n",
    "                                      max_depth=spec_params[name][\"randomforestclassifier__max_depth\"],\n",
    "                                      max_features=spec_params[name][\"randomforestclassifier__max_features\"],\n",
    "                                      min_samples_leaf=spec_params[name][\"randomforestclassifier__min_samples_leaf\"],\n",
    "                                      min_samples_split=spec_params[name][\"randomforestclassifier__min_samples_split\"],\n",
    "                                      n_estimators=spec_params[name][\"randomforestclassifier__n_estimators\"])\n",
    "            elif modelname[name] == \"XGB\":\n",
    "                model_temp = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=random_state)\n",
    "                model_temp.set_params(colsample_bytree=spec_params[name][\"xgbclassifier__colsample_bytree\"],\n",
    "                                      eta=spec_params[name][\"xgbclassifier__eta\"],\n",
    "                                      gamma=spec_params[name][\"xgbclassifier__gamma\"],\n",
    "                                      max_depth=spec_params[name][\"xgbclassifier__max_depth\"],\n",
    "                                      min_child_weight=spec_params[name][\"xgbclassifier__min_child_weight\"],\n",
    "                                      subsample=spec_params[name][\"xgbclassifier__subsample\"])\n",
    "            elif modelname[name] == \"VotingClassifier\":\n",
    "                \n",
    "                # Spec params must be the list of the dictionaries with the params in order (SVC - RF - XGB)\n",
    "                model_svc = SVC(random_state=random_state, probability=True)\n",
    "                model_rf = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "                model_xgb = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=random_state)\n",
    "\n",
    "                model_svc.set_params(C=spec_params[0][name][\"svc__C\"],\n",
    "                                     gamma=spec_params[0][name][\"svc__gamma\"],\n",
    "                                     kernel=spec_params[0][name][\"svc__kernel\"])\n",
    "                model_rf.set_params(bootstrap=spec_params[1][name][\"randomforestclassifier__bootstrap\"],\n",
    "                                    max_depth=spec_params[1][name][\"randomforestclassifier__max_depth\"],\n",
    "                                    max_features=spec_params[1][name][\"randomforestclassifier__max_features\"],\n",
    "                                    min_samples_leaf=spec_params[1][name][\"randomforestclassifier__min_samples_leaf\"],\n",
    "                                    min_samples_split=spec_params[1][name][\"randomforestclassifier__min_samples_split\"],\n",
    "                                    n_estimators=spec_params[1][name][\"randomforestclassifier__n_estimators\"])\n",
    "                model_xgb.set_params(colsample_bytree=spec_params[2][name][\"xgbclassifier__colsample_bytree\"],\n",
    "                                     eta=spec_params[2][name][\"xgbclassifier__eta\"],\n",
    "                                     gamma=spec_params[2][name][\"xgbclassifier__gamma\"],\n",
    "                                     max_depth=spec_params[2][name][\"xgbclassifier__max_depth\"],\n",
    "                                     min_child_weight=spec_params[2][name][\"xgbclassifier__min_child_weight\"],\n",
    "                                     subsample=spec_params[2][name][\"xgbclassifier__subsample\"])\n",
    "\n",
    "                model_temp = VotingClassifier(estimators=[(\"svc\", model_svc), (\"rf\", model_rf), (\"xgb\", model_xgb)],\n",
    "                                              voting=\"soft\", n_jobs=n_jobs)\n",
    "\n",
    "            else:\n",
    "                print(\"Please specify used model (SVC, RF, XGB)\")\n",
    "                return None\n",
    "            scores = cv_report(model_temp, X_train_dic[name], y_train[name], balancing=balancing, n_splits=n_splits,\n",
    "                               scoring_metrics=scoring_metrics, n_jobs=n_jobs, verbose=verbose,\n",
    "                               random_state=random_state)\n",
    "        else:\n",
    "            scores = cv_report(model, X_train_dic[name], y_train[name], balancing=balancing, n_splits=n_splits,\n",
    "                               scoring_metrics=scoring_metrics, n_jobs=n_jobs, verbose=verbose,\n",
    "                               random_state=random_state)\n",
    "        report.loc[name, \"F1 Micro\"] = round(float(scores[\"f1_micr_score\"]), 3)\n",
    "        report.loc[name, \"F1 Macro\"] = round(float(scores[\"f1_macro_score\"]), 3)\n",
    "        report.loc[name, \"F1 Binary\"] = round(float(scores[\"f1_score\"]), 3)\n",
    "        report.loc[name, \"ROC_AUC\"] = round(float(scores[\"auc_score\"]), 3)\n",
    "        report.loc[name, \"Recall\"] = round(float(scores[\"rec_score\"]), 3)\n",
    "        report.loc[name, \"Precision\"] = round(float(scores[\"prec_score\"]), 3)\n",
    "        report.loc[name, \"Average Precision\"] = round(float(scores[\"avp_score\"]), 3)\n",
    "    report = report.apply(pd.to_numeric)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "w9dUsDs2rAr2"
   },
   "outputs": [],
   "source": [
    "def test_score_multi_report(X_train_dic, y_train, X_test, y_test, out_names, model=None, modelname=None,\n",
    "                            spec_params=None, balancing=False, random_state=None, plot=False, verbose=False, n_jobs=-1):\n",
    "    \n",
    "    # Creates a scores report dataframe for each classification label with cv\n",
    "    # Initizalize the dataframe\n",
    "    report = pd.DataFrame(columns=[\"F1 Binary\", \"F1 Micro\", \"F1 Macro\", \"ROC_AUC\", \"Recall\", \"Precision\"],\n",
    "                          index=out_names)\n",
    "\n",
    "    # For each label\n",
    "    for name in tqdm(out_names):\n",
    "        if verbose:\n",
    "            print()\n",
    "            print(f\"Scores for {name}\")\n",
    "        \n",
    "        # Calculate the score for the current label using the respective dataframe\n",
    "        if spec_params:\n",
    "            \n",
    "            # Define the specific parameters for each model for each label\n",
    "            if modelname[name] == \"SVC\":\n",
    "                model_temp = SVC(random_state=random_state, probability=True)\n",
    "                model_temp.set_params(C=spec_params[name][\"svc__C\"],\n",
    "                                      gamma=spec_params[name][\"svc__gamma\"],\n",
    "                                      kernel=spec_params[name][\"svc__kernel\"])\n",
    "            elif modelname[name] == \"RF\":\n",
    "                model_temp = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "                model_temp.set_params(bootstrap=spec_params[name][\"randomforestclassifier__bootstrap\"],\n",
    "                                      max_depth=spec_params[name][\"randomforestclassifier__max_depth\"],\n",
    "                                      max_features=spec_params[name][\"randomforestclassifier__max_features\"],\n",
    "                                      min_samples_leaf=spec_params[name][\"randomforestclassifier__min_samples_leaf\"],\n",
    "                                      min_samples_split=spec_params[name][\"randomforestclassifier__min_samples_split\"],\n",
    "                                      n_estimators=spec_params[name][\"randomforestclassifier__n_estimators\"])\n",
    "            elif modelname[name] == \"XGB\":\n",
    "                model_temp = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=random_state)\n",
    "                model_temp.set_params(colsample_bytree=spec_params[name][\"xgbclassifier__colsample_bytree\"],\n",
    "                                      eta=spec_params[name][\"xgbclassifier__eta\"],\n",
    "                                      gamma=spec_params[name][\"xgbclassifier__gamma\"],\n",
    "                                      max_depth=spec_params[name][\"xgbclassifier__max_depth\"],\n",
    "                                      min_child_weight=spec_params[name][\"xgbclassifier__min_child_weight\"],\n",
    "                                      subsample=spec_params[name][\"xgbclassifier__subsample\"])\n",
    "            else:\n",
    "                print(\"Please specify used model (SVC, RF, XGB)\")\n",
    "                return None\n",
    "\n",
    "            if balancing:\n",
    "                \n",
    "                # Save index of categorical features\n",
    "                cat_shape = np.full((1128,), True, dtype=bool)\n",
    "                cat_shape[-3:] = False\n",
    "                \n",
    "                # Prepatre SMOTENC\n",
    "                smotenc = SMOTENC(categorical_features=cat_shape, random_state=random_state, n_jobs=n_jobs)\n",
    "                \n",
    "                # Make a pipeline with the balancing and the estimator, balacing is only called when fitting\n",
    "                pipeline = make_pipeline(smotenc, model_temp)\n",
    "                \n",
    "                # Fit and test\n",
    "                pipeline.fit(np.asarray(X_train_dic[name]), np.asarray(y_train[name]))\n",
    "                scores = score_report(pipeline, np.asarray(X_test[name]), np.asarray(y_test[name]), plot=plot,\n",
    "                                      verbose=verbose, name=name)\n",
    "\n",
    "            else:\n",
    "                model_temp.fit(np.asarray(X_train_dic[name]), np.asarray(y_train[name]))\n",
    "                scores = score_report(model_temp, np.asarray(X_test[name]), np.asarray(y_test[name]), plot=plot,\n",
    "                                      verbose=verbose, name=name)\n",
    "\n",
    "        else:\n",
    "            if balancing:\n",
    "                \n",
    "                # Save index of categorical features\n",
    "                cat_shape = np.full((1128,), True, dtype=bool)\n",
    "                cat_shape[-3:] = False\n",
    "                \n",
    "                # Prepatre SMOTENC\n",
    "                smotenc = SMOTENC(categorical_features=cat_shape, random_state=random_state, n_jobs=n_jobs)\n",
    "\n",
    "                # Make a pipeline with the balancing and the estimator, balacing is only called when fitting\n",
    "                pipeline = make_pipeline(smotenc, model)\n",
    "                \n",
    "                # Fit and test\n",
    "                pipeline.fit(np.asarray(X_train_dic[name]), np.asarray(y_train[name]))\n",
    "                scores = score_report(pipeline, np.asarray(X_test[name]), np.asarray(y_test[name]), plot=plot,\n",
    "                                      verbose=verbose, name=name)\n",
    "\n",
    "            else:\n",
    "                model.fit(np.asarray(X_train_dic[name]), np.asarray(y_train[name]))\n",
    "                scores = score_report(model, np.asarray(X_test[name]), np.asarray(y_test[name]), plot=plot,\n",
    "                                      verbose=verbose, name=name)\n",
    "\n",
    "        report.loc[name, \"F1 Micro\"] = round(float(scores[\"f1_micr_score\"]), 3)\n",
    "        report.loc[name, \"F1 Macro\"] = round(float(scores[\"f1_macro_score\"]), 3)\n",
    "        report.loc[name, \"F1 Binary\"] = round(float(scores[\"f1_s_score\"]), 3)\n",
    "        report.loc[name, \"ROC_AUC\"] = round(float(scores[\"auc_score\"]), 3)\n",
    "        report.loc[name, \"Recall\"] = round(float(scores[\"rec_score\"]), 3)\n",
    "        report.loc[name, \"Precision\"] = round(float(scores[\"prec_score\"]), 3)\n",
    "        report.loc[name, \"Average Prec-Rec\"] = round(float(scores[\"prec_rec_score\"]), 3)\n",
    "        \n",
    "        # prec_rec_score\n",
    "    report = report.apply(pd.to_numeric)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "VWlFDqWMrNOx"
   },
   "outputs": [],
   "source": [
    "def get_smile_from_cid(cid):\n",
    "    # Trim CID\n",
    "    ct = re.sub(\"^CID[0]*\", \"\", cid)\n",
    "\n",
    "    # Getting smile\n",
    "    res = requests.get(f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{ct}/property/CanonicalSMILES/txt\")\n",
    "\n",
    "    # Checking for Error 400\n",
    "    try:\n",
    "        res.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Problem retrieving smile for {cid}: {e}\")\n",
    "\n",
    "    # If everything is ok, get smile text\n",
    "    res_t = res.text.strip(\"\\n\")\n",
    "\n",
    "    # Return smile\n",
    "    return res_t"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
